{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling & Pre-processing Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start downloading... 30 tweets\n",
      "Tweets has been saved: 30\n",
      "Done! 30 tweets saved at \"tweepy jogja.json\"\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tweepy as tw\n",
    "import sys,jsonpickle\n",
    "\n",
    "# find it yourself okay?\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    " \n",
    "qry='jogja'\n",
    "maxTweets = 30\n",
    "tweetsPerQry = 30\n",
    "fName='tweepy ' +qry+ '.json'\n",
    " \n",
    "auth = tw.AppAuthHandler(consumer_key,consumer_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "if (not api):\n",
    "    sys.exit('Authentication failed')\n",
    " \n",
    "sinceId=None;max_id=-1;tweetCount=0\n",
    "print(\"Start downloading... {0} tweets\".format(maxTweets))\n",
    "with open(fName,'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets=api.search(q=qry,count=tweetsPerQry,result_type=\"popular\")\n",
    "                else:\n",
    "                    new_tweets=api.search(q=qry,count=tweetsPerQry,since_id=sinceId,result_type=\"popular\")\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets=api.search(q=qry,count=tweetsPerQry,max_id=str(max_id - 1),result_type=\"popular\")\n",
    "                else:\n",
    "                    new_tweets=api.search(q=qry,count=tweetsPerQry,max_id=str(max_id - 1),since_id=sinceId,result_type=\"popular\")\n",
    "            if not new_tweets:\n",
    "                print('No more Tweets found with Query=\"{0}\"'.format(qry));break\n",
    "            for tweet in new_tweets:\n",
    "                f.write(jsonpickle.encode(tweet._json,unpicklable=False)+'\\n')\n",
    "            tweetCount+=len(new_tweets)\n",
    "            sys.stdout.write(\"\\r\");sys.stdout.write(\"Tweets has been saved: %.0f\" %tweetCount);sys.stdout.flush()\n",
    "            max_id=new_tweets[-1].id\n",
    "        except tw.TweepError as e:\n",
    "            print(\"some error : \" + str(e));break # Aya error, keluar\n",
    "print ('\\nDone! {0} tweets saved at \"{1}\"'.format(tweetCount,fName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json,re\n",
    "file = open('tweepy ' +qry+ '.json') \n",
    "lines = file.readlines()\n",
    "\n",
    "hasil=open(\"tweepy_preprocessed.csv\",'w')\n",
    "for i in lines:\n",
    "#     print(json.loads(i.strip())['text'].lower())\n",
    "    tweet = json.loads(i.strip())['text'].lower()\n",
    "    tweet = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*','', tweet).replace('\\n', ' ')\n",
    "    hasil.write(str(tweet.encode('utf-8'))+'\\n')\n",
    "    \n",
    "hasil.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = pd.read_table(\"Book7.csv\", header=None, names=['tweet','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"Book7.csv\", sep=';', header=None, names=[\"text\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breaking news : pss sleman tahan imbang kalten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wkt pulang dr jogja mau share foto ini tapi lu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>link live streaming useetv liga 2 2018 kalteng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siaran pers 24 november 2018, mengenai kejadia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>breaking news : waspada, empat kali guguran la...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pelatih pss sleman syukuri hasil imbang lawan ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>selamat hari guru nasional. tetaplah menjadi p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>keutamaan dan doa puasa sunnah senin kamis bag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pss sleman tahan imbang kalteng putra di palan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>raih satu poin di kandang kalteng putra, ini k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pesawat garuda indonesia tergelincir di ujung ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kalteng putra vs pss sleman, cristian gonzales...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>berikut hari libur nasional &amp;amp; cuti bersama...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bagi yg nembakin burung, kasihan woiii ... jog...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ribuan burung layang-layang asia bermigrasi ke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kita sampai jogja sekitar jam 16.30!  bakal ad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>di jogja ngapain aja dari kemarin? @cindvia_jk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>maturnuwun nggih jogja buat 2 hari yang menyen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>press conference pemilihan member single ke-20...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ketemu di jogja city mall yaaa guyz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sebentar lagi sampai jogja!  nanti ada sesuatu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>otw jogja! #jkt48ontrain @kai121 @keretaapikita</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lur, terutama di jateng, jogja dan khususnya m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>daftar 10 warung mie ayam murah dan enak di yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>thankyou jogja. bisa ke jogja lagi nggak ya? i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>thank you untuk pre event nya di jogja! semang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ini ada video.. yang waktu itu.. hehehe part 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>susu untuk hari yang lebi baik~ sekarang lagi ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>both shoplifters and one cut of the dead will ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>di kereta sebelahan bangku sama dik peni. peng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0   breaking news : pss sleman tahan imbang kalten...      1\n",
       "1   wkt pulang dr jogja mau share foto ini tapi lu...      0\n",
       "2   link live streaming useetv liga 2 2018 kalteng...      0\n",
       "3   siaran pers 24 november 2018, mengenai kejadia...      0\n",
       "4   breaking news : waspada, empat kali guguran la...     -1\n",
       "5   pelatih pss sleman syukuri hasil imbang lawan ...      1\n",
       "6   selamat hari guru nasional. tetaplah menjadi p...      1\n",
       "7   keutamaan dan doa puasa sunnah senin kamis bag...      1\n",
       "8   pss sleman tahan imbang kalteng putra di palan...      1\n",
       "9   raih satu poin di kandang kalteng putra, ini k...      1\n",
       "10  pesawat garuda indonesia tergelincir di ujung ...     -1\n",
       "11  kalteng putra vs pss sleman, cristian gonzales...     -1\n",
       "12  berikut hari libur nasional &amp; cuti bersama...      1\n",
       "13  bagi yg nembakin burung, kasihan woiii ... jog...     -1\n",
       "14  ribuan burung layang-layang asia bermigrasi ke...      0\n",
       "15  kita sampai jogja sekitar jam 16.30!  bakal ad...      0\n",
       "16  di jogja ngapain aja dari kemarin? @cindvia_jk...      0\n",
       "17  maturnuwun nggih jogja buat 2 hari yang menyen...      1\n",
       "18  press conference pemilihan member single ke-20...      0\n",
       "19               ketemu di jogja city mall yaaa guyz       0\n",
       "20  sebentar lagi sampai jogja!  nanti ada sesuatu...      0\n",
       "21   otw jogja! #jkt48ontrain @kai121 @keretaapikita       0\n",
       "22  lur, terutama di jateng, jogja dan khususnya m...      0\n",
       "23  daftar 10 warung mie ayam murah dan enak di yo...      1\n",
       "24  thankyou jogja. bisa ke jogja lagi nggak ya? i...      1\n",
       "25  thank you untuk pre event nya di jogja! semang...      1\n",
       "26  ini ada video.. yang waktu itu.. hehehe part 1...      0\n",
       "27  susu untuk hari yang lebi baik~ sekarang lagi ...      0\n",
       "28  both shoplifters and one cut of the dead will ...      0\n",
       "29  di kereta sebelahan bangku sama dik peni. peng...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    15\n",
       " 1    11\n",
       "-1     4\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "X = tweets.text\n",
    "y = tweets.label\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27,)\n",
      "(3,)\n",
      "(27,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(tweets)\n",
    "# vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 997 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=2, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " google\n",
      " cat\n",
      " best\n",
      " climbing\n",
      " ninja\n",
      " incredible\n",
      " app\n",
      " translate\n",
      " impressed\n",
      " map\n",
      "Cluster 1:\n",
      " eating\n",
      " kitty\n",
      " little\n",
      " came\n",
      " restaurant\n",
      " play\n",
      " ve\n",
      " feedback\n",
      " face\n",
      " extension\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction [0]\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.transform([\"chrome browser to open.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(\"Prediction\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction [0]\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.transform([\"My cat is hungry.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(\"Prediction\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction [0]\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.transform([\"photo restaurant mcdonald\"])\n",
    "prediction = model.predict(Y)\n",
    "print(\"Prediction\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction [0]\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.transform([\"take your photo off the wall\"])\n",
    "prediction = model.predict(Y)\n",
    "print(\"Prediction\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
