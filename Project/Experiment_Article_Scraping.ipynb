{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from newspaper import Article\n",
    "\n",
    "host = 'localhost'\n",
    "port = '3306'\n",
    "username = 'root'\n",
    "password = ''\n",
    "database = 'gdelt_content_id'\n",
    "\n",
    "# Create Connection to database\n",
    "# engine = create_engine('mysql+pymysql://'+username+\n",
    "# ':'+password+'@'+host+':'+port+'/'+database)\n",
    "\n",
    "# Note: We use pymysql instead of sqlalchemy because sqlalchemy\n",
    "# somehow don't allow the text query. Strange bug.\n",
    "conn = pymysql.connect(\n",
    "    host=host,\n",
    "    port=int(port),\n",
    "    user=username,\n",
    "    passwd=password,\n",
    "    db=database,\n",
    "    charset='utf8mb4')\n",
    "'''engine = create_engine('mysql+pymysql://root: @localhost:3306\n",
    "/gdelt_content_id')'''\n",
    "\n",
    "def run(sql):\n",
    "    df = pd.read_sql_query(sql, conn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_startup = run(\"SELECT GLOBALEVENTID, SQLDATE, Year, SOURCEURL FROM startup_indonesia WHERE year = 2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_baris = data_startup.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping article 1 of 402\n",
      "Scraping article 2 of 402\n",
      "Scraping article 3 of 402\n",
      "Scraping article 4 of 402\n",
      "Scraping article 5 of 402\n",
      "Scraping article 6 of 402\n",
      "Scraping article 7 of 402\n",
      "Scraping article 8 of 402\n",
      "Scraping article 9 of 402\n",
      "Scraping article 10 of 402\n",
      "Scraping article 11 of 402\n",
      "Scraping article 12 of 402\n",
      "Scraping article 13 of 402\n",
      "Scraping article 14 of 402\n",
      "Scraping article 15 of 402\n",
      "Scraping article 16 of 402\n",
      "Scraping article 17 of 402\n",
      "Scraping article 18 of 402\n",
      "Scraping article 19 of 402\n",
      "Scraping article 20 of 402\n",
      "Scraping article 21 of 402\n",
      "Scraping article 22 of 402\n",
      "Scraping article 23 of 402\n",
      "Scraping article 24 of 402\n",
      "Scraping article 25 of 402\n",
      "Scraping article 26 of 402\n",
      "Scraping article 27 of 402\n",
      "Scraping article 28 of 402\n",
      "Scraping article 29 of 402\n",
      "Scraping article 30 of 402\n",
      "Scraping article 31 of 402\n",
      "Scraping article 32 of 402\n",
      "Scraping article 33 of 402\n",
      "Scraping article 34 of 402\n",
      "Scraping article 35 of 402\n",
      "Scraping article 36 of 402\n",
      "Scraping article 37 of 402\n",
      "Scraping article 38 of 402\n",
      "Scraping article 39 of 402\n",
      "Scraping article 40 of 402\n",
      "Scraping article 41 of 402\n",
      "Scraping article 42 of 402\n",
      "Scraping article 43 of 402\n",
      "Scraping article 44 of 402\n",
      "Scraping article 45 of 402\n",
      "Scraping article 46 of 402\n",
      "Scraping article 47 of 402\n",
      "Scraping article 48 of 402\n",
      "Scraping article 49 of 402\n",
      "Scraping article 50 of 402\n",
      "Scraping article 51 of 402\n",
      "Scraping article 52 of 402\n",
      "Scraping article 53 of 402\n",
      "Scraping article 54 of 402\n",
      "Scraping article 55 of 402\n",
      "Scraping article 56 of 402\n",
      "Scraping article 57 of 402\n",
      "Scraping article 58 of 402\n",
      "Scraping article 59 of 402\n",
      "Scraping article 60 of 402\n",
      "Scraping article 61 of 402\n",
      "Scraping article 62 of 402\n",
      "Scraping article 63 of 402\n",
      "Scraping article 64 of 402\n",
      "Error scraping article 64 of 402\n",
      "Scraping article 65 of 402\n",
      "Scraping article 66 of 402\n",
      "Scraping article 67 of 402\n",
      "Scraping article 68 of 402\n",
      "Scraping article 69 of 402\n",
      "Scraping article 70 of 402\n",
      "Scraping article 71 of 402\n",
      "Scraping article 72 of 402\n",
      "Scraping article 73 of 402\n",
      "Scraping article 74 of 402\n",
      "Scraping article 75 of 402\n",
      "Scraping article 76 of 402\n",
      "Scraping article 77 of 402\n",
      "Scraping article 78 of 402\n",
      "Scraping article 79 of 402\n",
      "Scraping article 80 of 402\n",
      "Scraping article 81 of 402\n",
      "Scraping article 82 of 402\n",
      "Scraping article 83 of 402\n",
      "Scraping article 84 of 402\n",
      "Scraping article 85 of 402\n",
      "Scraping article 86 of 402\n",
      "Scraping article 87 of 402\n",
      "Scraping article 88 of 402\n",
      "Scraping article 89 of 402\n",
      "Scraping article 90 of 402\n",
      "Scraping article 91 of 402\n",
      "Scraping article 92 of 402\n",
      "Scraping article 93 of 402\n",
      "Scraping article 94 of 402\n",
      "Scraping article 95 of 402\n",
      "Scraping article 96 of 402\n",
      "Scraping article 97 of 402\n",
      "Scraping article 98 of 402\n",
      "Scraping article 99 of 402\n",
      "Scraping article 100 of 402\n",
      "Scraping article 101 of 402\n",
      "Error scraping article 101 of 402\n",
      "Scraping article 102 of 402\n",
      "Scraping article 103 of 402\n",
      "Scraping article 104 of 402\n",
      "Error scraping article 104 of 402\n",
      "Scraping article 105 of 402\n",
      "Scraping article 106 of 402\n",
      "Scraping article 107 of 402\n",
      "Scraping article 108 of 402\n",
      "Scraping article 109 of 402\n",
      "Scraping article 110 of 402\n",
      "Scraping article 111 of 402\n",
      "Scraping article 112 of 402\n",
      "Scraping article 113 of 402\n",
      "Scraping article 114 of 402\n",
      "Scraping article 115 of 402\n",
      "Scraping article 116 of 402\n",
      "Scraping article 117 of 402\n",
      "Scraping article 118 of 402\n",
      "Scraping article 119 of 402\n",
      "Scraping article 120 of 402\n",
      "Scraping article 121 of 402\n",
      "Scraping article 122 of 402\n",
      "Scraping article 123 of 402\n",
      "Scraping article 124 of 402\n",
      "Scraping article 125 of 402\n",
      "Scraping article 126 of 402\n",
      "Scraping article 127 of 402\n",
      "Scraping article 128 of 402\n",
      "Scraping article 129 of 402\n",
      "Scraping article 130 of 402\n",
      "Scraping article 131 of 402\n",
      "Scraping article 132 of 402\n",
      "Scraping article 133 of 402\n",
      "Scraping article 134 of 402\n",
      "Scraping article 135 of 402\n",
      "Scraping article 136 of 402\n",
      "Scraping article 137 of 402\n",
      "Scraping article 138 of 402\n",
      "Scraping article 139 of 402\n",
      "Error scraping article 139 of 402\n",
      "Scraping article 140 of 402\n",
      "Scraping article 141 of 402\n",
      "Scraping article 142 of 402\n",
      "Scraping article 143 of 402\n",
      "Scraping article 144 of 402\n",
      "Scraping article 145 of 402\n",
      "Scraping article 146 of 402\n",
      "Scraping article 147 of 402\n",
      "Scraping article 148 of 402\n",
      "Scraping article 149 of 402\n",
      "Scraping article 150 of 402\n",
      "Scraping article 151 of 402\n",
      "Scraping article 152 of 402\n",
      "Scraping article 153 of 402\n",
      "Scraping article 154 of 402\n",
      "Scraping article 155 of 402\n",
      "Scraping article 156 of 402\n",
      "Scraping article 157 of 402\n",
      "Error scraping article 157 of 402\n",
      "Scraping article 158 of 402\n",
      "Scraping article 159 of 402\n",
      "Scraping article 160 of 402\n",
      "Scraping article 161 of 402\n",
      "Scraping article 162 of 402\n",
      "Scraping article 163 of 402\n",
      "Scraping article 164 of 402\n",
      "Scraping article 165 of 402\n",
      "Scraping article 166 of 402\n",
      "Error scraping article 166 of 402\n",
      "Scraping article 167 of 402\n",
      "Scraping article 168 of 402\n",
      "Scraping article 169 of 402\n",
      "Scraping article 170 of 402\n",
      "Scraping article 171 of 402\n",
      "Scraping article 172 of 402\n",
      "Scraping article 173 of 402\n",
      "Scraping article 174 of 402\n",
      "Scraping article 175 of 402\n",
      "Scraping article 176 of 402\n",
      "Scraping article 177 of 402\n",
      "Scraping article 178 of 402\n",
      "Scraping article 179 of 402\n",
      "Scraping article 180 of 402\n",
      "Scraping article 181 of 402\n",
      "Scraping article 182 of 402\n",
      "Scraping article 183 of 402\n",
      "Scraping article 184 of 402\n",
      "Scraping article 185 of 402\n",
      "Scraping article 186 of 402\n",
      "Scraping article 187 of 402\n",
      "Scraping article 188 of 402\n",
      "Scraping article 189 of 402\n",
      "Scraping article 190 of 402\n",
      "Scraping article 191 of 402\n",
      "Scraping article 192 of 402\n",
      "Scraping article 193 of 402\n",
      "Scraping article 194 of 402\n",
      "Scraping article 195 of 402\n",
      "Scraping article 196 of 402\n",
      "Scraping article 197 of 402\n",
      "Scraping article 198 of 402\n",
      "Scraping article 199 of 402\n",
      "Scraping article 200 of 402\n",
      "Scraping article 201 of 402\n",
      "Scraping article 202 of 402\n",
      "Scraping article 203 of 402\n",
      "Scraping article 204 of 402\n",
      "Scraping article 205 of 402\n",
      "Scraping article 206 of 402\n",
      "Scraping article 207 of 402\n",
      "Scraping article 208 of 402\n",
      "Scraping article 209 of 402\n",
      "Scraping article 210 of 402\n",
      "Scraping article 211 of 402\n",
      "Scraping article 212 of 402\n",
      "Scraping article 213 of 402\n",
      "Scraping article 214 of 402\n",
      "Scraping article 215 of 402\n",
      "Scraping article 216 of 402\n",
      "Scraping article 217 of 402\n",
      "Scraping article 218 of 402\n",
      "Scraping article 219 of 402\n",
      "Scraping article 220 of 402\n",
      "Scraping article 221 of 402\n",
      "Scraping article 222 of 402\n",
      "Scraping article 223 of 402\n",
      "Scraping article 224 of 402\n",
      "Scraping article 225 of 402\n",
      "Scraping article 226 of 402\n",
      "Scraping article 227 of 402\n",
      "Error scraping article 227 of 402\n",
      "Scraping article 228 of 402\n",
      "Scraping article 229 of 402\n",
      "Scraping article 230 of 402\n",
      "Scraping article 231 of 402\n",
      "Scraping article 232 of 402\n",
      "Scraping article 233 of 402\n",
      "Scraping article 234 of 402\n",
      "Scraping article 235 of 402\n",
      "Error scraping article 235 of 402\n",
      "Scraping article 236 of 402\n",
      "Scraping article 237 of 402\n",
      "Scraping article 238 of 402\n",
      "Scraping article 239 of 402\n",
      "Scraping article 240 of 402\n",
      "Scraping article 241 of 402\n",
      "Scraping article 242 of 402\n",
      "Error scraping article 242 of 402\n",
      "Scraping article 243 of 402\n",
      "Scraping article 244 of 402\n",
      "Scraping article 245 of 402\n",
      "Scraping article 246 of 402\n",
      "Scraping article 247 of 402\n",
      "Scraping article 248 of 402\n",
      "Scraping article 249 of 402\n",
      "Scraping article 250 of 402\n",
      "Scraping article 251 of 402\n",
      "Scraping article 252 of 402\n",
      "Scraping article 253 of 402\n",
      "Scraping article 254 of 402\n",
      "Scraping article 255 of 402\n",
      "Scraping article 256 of 402\n",
      "Scraping article 257 of 402\n",
      "Scraping article 258 of 402\n",
      "Scraping article 259 of 402\n",
      "Scraping article 260 of 402\n",
      "Scraping article 261 of 402\n",
      "Scraping article 262 of 402\n",
      "Scraping article 263 of 402\n",
      "Scraping article 264 of 402\n",
      "Scraping article 265 of 402\n",
      "Scraping article 266 of 402\n",
      "Scraping article 267 of 402\n",
      "Scraping article 268 of 402\n",
      "Scraping article 269 of 402\n",
      "Scraping article 270 of 402\n",
      "Scraping article 271 of 402\n",
      "Scraping article 272 of 402\n",
      "Scraping article 273 of 402\n",
      "Scraping article 274 of 402\n",
      "Scraping article 275 of 402\n",
      "Scraping article 276 of 402\n",
      "Scraping article 277 of 402\n",
      "Scraping article 278 of 402\n",
      "Scraping article 279 of 402\n",
      "Scraping article 280 of 402\n",
      "Scraping article 281 of 402\n",
      "Scraping article 282 of 402\n",
      "Scraping article 283 of 402\n",
      "Scraping article 284 of 402\n",
      "Scraping article 285 of 402\n",
      "Scraping article 286 of 402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping article 287 of 402\n",
      "Scraping article 288 of 402\n",
      "Error scraping article 288 of 402\n",
      "Scraping article 289 of 402\n",
      "Scraping article 290 of 402\n",
      "Scraping article 291 of 402\n",
      "Scraping article 292 of 402\n",
      "Scraping article 293 of 402\n",
      "Scraping article 294 of 402\n",
      "Scraping article 295 of 402\n",
      "Scraping article 296 of 402\n",
      "Scraping article 297 of 402\n",
      "Scraping article 298 of 402\n",
      "Scraping article 299 of 402\n",
      "Scraping article 300 of 402\n",
      "Scraping article 301 of 402\n",
      "Scraping article 302 of 402\n",
      "Scraping article 303 of 402\n",
      "Scraping article 304 of 402\n",
      "Scraping article 305 of 402\n",
      "Scraping article 306 of 402\n",
      "Scraping article 307 of 402\n",
      "Scraping article 308 of 402\n",
      "Scraping article 309 of 402\n",
      "Scraping article 310 of 402\n",
      "Scraping article 311 of 402\n",
      "Scraping article 312 of 402\n",
      "Scraping article 313 of 402\n",
      "Scraping article 314 of 402\n",
      "Scraping article 315 of 402\n",
      "Scraping article 316 of 402\n",
      "Error scraping article 316 of 402\n",
      "Scraping article 317 of 402\n",
      "Scraping article 318 of 402\n",
      "Scraping article 319 of 402\n",
      "Scraping article 320 of 402\n",
      "Scraping article 321 of 402\n",
      "Scraping article 322 of 402\n",
      "Scraping article 323 of 402\n",
      "Scraping article 324 of 402\n",
      "Scraping article 325 of 402\n",
      "Error scraping article 325 of 402\n",
      "Scraping article 326 of 402\n",
      "Scraping article 327 of 402\n",
      "Error scraping article 327 of 402\n",
      "Scraping article 328 of 402\n",
      "Scraping article 329 of 402\n",
      "Error scraping article 329 of 402\n",
      "Scraping article 330 of 402\n",
      "Error scraping article 330 of 402\n",
      "Scraping article 331 of 402\n",
      "Scraping article 332 of 402\n",
      "Scraping article 333 of 402\n",
      "Scraping article 334 of 402\n",
      "Scraping article 335 of 402\n",
      "Scraping article 336 of 402\n",
      "Scraping article 337 of 402\n",
      "Scraping article 338 of 402\n",
      "Scraping article 339 of 402\n",
      "Scraping article 340 of 402\n",
      "Scraping article 341 of 402\n",
      "Scraping article 342 of 402\n",
      "Scraping article 343 of 402\n",
      "Scraping article 344 of 402\n",
      "Scraping article 345 of 402\n",
      "Scraping article 346 of 402\n",
      "Scraping article 347 of 402\n",
      "Scraping article 348 of 402\n",
      "Scraping article 349 of 402\n",
      "Scraping article 350 of 402\n",
      "Error scraping article 350 of 402\n",
      "Scraping article 351 of 402\n",
      "Scraping article 352 of 402\n",
      "Scraping article 353 of 402\n",
      "Scraping article 354 of 402\n",
      "Scraping article 355 of 402\n",
      "Scraping article 356 of 402\n",
      "Scraping article 357 of 402\n",
      "Scraping article 358 of 402\n",
      "Scraping article 359 of 402\n",
      "Scraping article 360 of 402\n",
      "Scraping article 361 of 402\n",
      "Scraping article 362 of 402\n",
      "Scraping article 363 of 402\n",
      "Scraping article 364 of 402\n",
      "Scraping article 365 of 402\n",
      "Scraping article 366 of 402\n",
      "Scraping article 367 of 402\n",
      "Scraping article 368 of 402\n",
      "Scraping article 369 of 402\n",
      "Scraping article 370 of 402\n",
      "Scraping article 371 of 402\n",
      "Scraping article 372 of 402\n",
      "Scraping article 373 of 402\n",
      "Scraping article 374 of 402\n",
      "Scraping article 375 of 402\n",
      "Scraping article 376 of 402\n",
      "Error scraping article 376 of 402\n",
      "Scraping article 377 of 402\n",
      "Scraping article 378 of 402\n",
      "Scraping article 379 of 402\n",
      "Scraping article 380 of 402\n",
      "Scraping article 381 of 402\n",
      "Scraping article 382 of 402\n",
      "Scraping article 383 of 402\n",
      "Scraping article 384 of 402\n",
      "Error scraping article 384 of 402\n",
      "Scraping article 385 of 402\n",
      "Scraping article 386 of 402\n",
      "Scraping article 387 of 402\n",
      "Scraping article 388 of 402\n",
      "Scraping article 389 of 402\n",
      "Scraping article 390 of 402\n",
      "Error scraping article 390 of 402\n",
      "Scraping article 391 of 402\n",
      "Scraping article 392 of 402\n",
      "Scraping article 393 of 402\n",
      "Scraping article 394 of 402\n",
      "Scraping article 395 of 402\n",
      "Scraping article 396 of 402\n",
      "Scraping article 397 of 402\n",
      "Scraping article 398 of 402\n",
      "Scraping article 399 of 402\n",
      "Scraping article 400 of 402\n",
      "Scraping article 401 of 402\n",
      "Scraping article 402 of 402\n"
     ]
    }
   ],
   "source": [
    "isi_berita = list()\n",
    "i = 1\n",
    "for link in data_startup.SOURCEURL:\n",
    "    print(\"Scraping article \" + str(i) + \" of \" + str(total_baris))\n",
    "    berita = Article(link)\n",
    "    try:\n",
    "        berita.download()\n",
    "        berita.parse()\n",
    "        isi_berita.append(berita.text)\n",
    "    except:\n",
    "        print(\"Error scraping article \" + str(i) + \" of \" + str(total_baris))\n",
    "        isi_berita.append(link)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(isi_berita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(isi_berita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_startup['ARTICLEURL'] = pd.Series(isi_berita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLOBALEVENTID</th>\n",
       "      <th>SQLDATE</th>\n",
       "      <th>Year</th>\n",
       "      <th>SOURCEURL</th>\n",
       "      <th>ARTICLEURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>798274953</td>\n",
       "      <td>20181029</td>\n",
       "      <td>2018</td>\n",
       "      <td>http://fintechnews.sg/25313/blockchain/blockch...</td>\n",
       "      <td>Following in the steps of international counte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>798359666</td>\n",
       "      <td>20181029</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://www.dealstreetasia.com/stories/indones...</td>\n",
       "      <td>Indonesian modem rental service company Passpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>797650072</td>\n",
       "      <td>20181026</td>\n",
       "      <td>2018</td>\n",
       "      <td>http://www.thejakartapost.com/news/2018/10/26/...</td>\n",
       "      <td>The government has long planned to collect tax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>797801179</td>\n",
       "      <td>20181026</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://www.mediapost.com/publications/article...</td>\n",
       "      <td>by Dave Morgan , Featured Contributor, October...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>797376646</td>\n",
       "      <td>20181025</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://www.dealstreetasia.com/stories/indones...</td>\n",
       "      <td>Photo by rawpixel on Unsplash\\n\\n//es_subbox( ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GLOBALEVENTID   SQLDATE  Year  \\\n",
       "0      798274953  20181029  2018   \n",
       "1      798359666  20181029  2018   \n",
       "2      797650072  20181026  2018   \n",
       "3      797801179  20181026  2018   \n",
       "4      797376646  20181025  2018   \n",
       "\n",
       "                                           SOURCEURL  \\\n",
       "0  http://fintechnews.sg/25313/blockchain/blockch...   \n",
       "1  https://www.dealstreetasia.com/stories/indones...   \n",
       "2  http://www.thejakartapost.com/news/2018/10/26/...   \n",
       "3  https://www.mediapost.com/publications/article...   \n",
       "4  https://www.dealstreetasia.com/stories/indones...   \n",
       "\n",
       "                                          ARTICLEURL  \n",
       "0  Following in the steps of international counte...  \n",
       "1  Indonesian modem rental service company Passpo...  \n",
       "2  The government has long planned to collect tax...  \n",
       "3  by Dave Morgan , Featured Contributor, October...  \n",
       "4  Photo by rawpixel on Unsplash\\n\\n//es_subbox( ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_startup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLOBALEVENTID</th>\n",
       "      <th>SQLDATE</th>\n",
       "      <th>Year</th>\n",
       "      <th>SOURCEURL</th>\n",
       "      <th>ARTICLEURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>788376323</td>\n",
       "      <td>20180919</td>\n",
       "      <td>2018</td>\n",
       "      <td>http://news.asiaone.com/singapore/agency-sold-...</td>\n",
       "      <td>http://news.asiaone.com/singapore/agency-sold-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>784607887</td>\n",
       "      <td>20180905</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://english.vietnamnet.vn/fms/business/207...</td>\n",
       "      <td>https://english.vietnamnet.vn/fms/business/207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>784110183</td>\n",
       "      <td>20180903</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://www.opengovasia.com/article/the-role-o...</td>\n",
       "      <td>https://www.opengovasia.com/article/the-role-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>778430938</td>\n",
       "      <td>20180810</td>\n",
       "      <td>2018</td>\n",
       "      <td>http://www.oann.com/adopt-a-unicorn-indonesia-...</td>\n",
       "      <td>http://www.oann.com/adopt-a-unicorn-indonesia-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>775115571</td>\n",
       "      <td>20180727</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://economictimes.indiatimes.com/small-biz...</td>\n",
       "      <td>https://economictimes.indiatimes.com/small-biz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GLOBALEVENTID   SQLDATE  Year  \\\n",
       "63       788376323  20180919  2018   \n",
       "100      784607887  20180905  2018   \n",
       "103      784110183  20180903  2018   \n",
       "138      778430938  20180810  2018   \n",
       "156      775115571  20180727  2018   \n",
       "\n",
       "                                             SOURCEURL  \\\n",
       "63   http://news.asiaone.com/singapore/agency-sold-...   \n",
       "100  https://english.vietnamnet.vn/fms/business/207...   \n",
       "103  https://www.opengovasia.com/article/the-role-o...   \n",
       "138  http://www.oann.com/adopt-a-unicorn-indonesia-...   \n",
       "156  https://economictimes.indiatimes.com/small-biz...   \n",
       "\n",
       "                                            ARTICLEURL  \n",
       "63   http://news.asiaone.com/singapore/agency-sold-...  \n",
       "100  https://english.vietnamnet.vn/fms/business/207...  \n",
       "103  https://www.opengovasia.com/article/the-role-o...  \n",
       "138  http://www.oann.com/adopt-a-unicorn-indonesia-...  \n",
       "156  https://economictimes.indiatimes.com/small-biz...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_incom = data_startup.loc[data_startup['ARTICLEURL'] == data_startup['SOURCEURL']]\n",
    "data_null = data_startup.loc[data_startup['ARTICLEURL'] == '']\n",
    "data_incom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLOBALEVENTID</th>\n",
       "      <th>SQLDATE</th>\n",
       "      <th>Year</th>\n",
       "      <th>SOURCEURL</th>\n",
       "      <th>ARTICLEURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>786997689</td>\n",
       "      <td>20180914</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://www.opengovasia.com/indonesia-furthers...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>782286669</td>\n",
       "      <td>20180827</td>\n",
       "      <td>2018</td>\n",
       "      <td>http://sglinks.com/startups/pages/118016605-go...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>782319837</td>\n",
       "      <td>20180827</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://www.thepaypers.com/mobile-payments/jd-...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>774333479</td>\n",
       "      <td>20180724</td>\n",
       "      <td>2018</td>\n",
       "      <td>http://sglinks.com/pages/116649609-tech-in-asi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>774019331</td>\n",
       "      <td>20180723</td>\n",
       "      <td>2018</td>\n",
       "      <td>http://sglinks.com/startups/pages/116605809-an...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GLOBALEVENTID   SQLDATE  Year  \\\n",
       "77       786997689  20180914  2018   \n",
       "126      782286669  20180827  2018   \n",
       "128      782319837  20180827  2018   \n",
       "162      774333479  20180724  2018   \n",
       "163      774019331  20180723  2018   \n",
       "\n",
       "                                             SOURCEURL ARTICLEURL  \n",
       "77   https://www.opengovasia.com/indonesia-furthers...             \n",
       "126  http://sglinks.com/startups/pages/118016605-go...             \n",
       "128  https://www.thepaypers.com/mobile-payments/jd-...             \n",
       "162  http://sglinks.com/pages/116649609-tech-in-asi...             \n",
       "163  http://sglinks.com/startups/pages/116605809-an...             "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69, 5)\n"
     ]
    }
   ],
   "source": [
    "data_incom = pd.concat([data_incom, data_null])\n",
    "data_incom.head()\n",
    "print(data_incom.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sglinks.com                     40\n",
       "www.opengovasia.com              7\n",
       "www.thepresidentpost.com         3\n",
       "economictimes.indiatimes.com     3\n",
       "www.chinamoneynetwork.com        3\n",
       "www.thepaypers.com               2\n",
       "www.gulf-times.com               2\n",
       "www.prnewswire.com               1\n",
       "sea-globe.com                    1\n",
       "news.asiaone.com                 1\n",
       "www.en.netralnews.com            1\n",
       "www.oann.com                     1\n",
       "english.vietnamnet.vn            1\n",
       "www.nytimes.com                  1\n",
       "som.yale.edu                     1\n",
       "www.theedgemarkets.com           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of troublesome domain link\n",
    "import urllib\n",
    "\n",
    "data_sourceurl = data_incom['SOURCEURL']\n",
    "list_link = []\n",
    "\n",
    "# for extracting domain url. Example: \n",
    "# http://fintechnews.sg/25313/blockchain/blockchain-indonesia/ --> fintechnews.sg\n",
    "for link in data_sourceurl:\n",
    "    list_link.append(urllib.parse.urlparse(link).netloc)\n",
    "    \n",
    "data_link = pd.Series(list_link)\n",
    "\n",
    "# Count by its occurences\n",
    "data_link_count = data_link.value_counts()\n",
    "data_link_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling sglinks.com\n",
    "def sglinks_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling opengovasia.com\n",
    "def opengovasia_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling thepresidentpost.com\n",
    "def thepresidentpost_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling economictimes.indiatimes.com\n",
    "def economictimes_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling chinamoneynetwork.com\n",
    "def chinamoneynetwork_scrape(link):\n",
    "    import urllib\n",
    "    r = urllib.request.Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    web_byte = urllib.request.urlopen(r).read()\n",
    "    webpage = web_byte.decode('utf-8')\n",
    "        \n",
    "    from bs4 import BeautifulSoup \n",
    "\n",
    "    soup = BeautifulSoup(r,\"lxml\")\n",
    "    article = soup.find('og:description')\n",
    "    print(article)\n",
    "    return article\n",
    "\n",
    "contoh = 'https://www.chinamoneynetwork.com/2018/07/18/chinese-fintech-start-up-advance-ai-raises-50m-series-b-round'\n",
    "chinamoneynetwork_scrape(contoh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling thepaypers.com\n",
    "def thepaypers_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling gulf-times.com\n",
    "def gulftimes_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-870a90ed25f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mcontoh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://www.prnewswire.com/news-releases/nuctech-participates-at-the-1st-global-cross-border-e-commerce-conference-300609935.html'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprnewswire_scrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontoh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-870a90ed25f6>\u001b[0m in \u001b[0;36mprnewswire_scrape\u001b[1;34m(link)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"section\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"release-body container\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# handling prnewswire.com\n",
    "def prnewswire_scrape(link):\n",
    "    import ssl\n",
    "    import urllib\n",
    "    \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    r = urllib.request.urlopen(link).read()\n",
    "    from bs4 import BeautifulSoup \n",
    "\n",
    "    soup = BeautifulSoup(r,\"lxml\")\n",
    "    article = soup.find(\"section\",\"release-body container\").text.strip()\n",
    "    return article\n",
    "\n",
    "contoh = 'https://www.prnewswire.com/news-releases/nuctech-participates-at-the-1st-global-cross-border-e-commerce-conference-300609935.html'\n",
    "print(prnewswire_scrape(contoh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling sea-globe.com\n",
    "def seaglobe_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling asiaone.com\n",
    "def asiaone_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \n",
      "\n",
      "SURABAYA, NNC - Surabaya Mayor Tri Rismaharini targets the City of Heroes as the first creative city in Indonesia through the Startup Nation Summit to be held in Surabaya, November 17-18, 2018.\n",
      "\"The Surabaya name is already well known in ASEAN and Asia but not yet [famous] in most European countries. This is the right momentum,\" Tri Rismaharini said in Surabaya on Sunday (3/11/2018).\n",
      "According to her, the 2018 Startup Nation Summit will look different from it was in previous years, because the City Government of Surabaya is planning to package the event with the today's atmosphere dubbed Innovation Creative industry.\n",
      "Risma said the Startup Nation Summit event which will be attended by 183 delegates from various countries will be collaborated with digital creative industry event. So it's not just an activity as usual, but will be flavored with digital and millennial atmosphere. The innovation creative event presents several agenda such as digital art, community activity, virtual reality, interactive games, innovation zone workshop and music performance.\n",
      "\"[We] deliberately present many agendas so that all ages could join in the event. We target one million visitors,\" she said.\n",
      "Another reason, Risma incorporated a creative digital agenda to realize Surabaya as the first creative city in Indonesia, advancing the startup world of Surabaya and bringing this city to the world map.\n",
      "In addition, she added, the city administration also promotes this event through several ways among others, promotional media, mass mobilization, billboards, flyers in schools, buses, villages and will feature in videotron (large screen).\n",
      "\"According to plan it starts later this month,\" she said.\n"
     ]
    }
   ],
   "source": [
    "# handling netralnews.com\n",
    "def netralnews_scrape(link):\n",
    "    import ssl\n",
    "    import urllib\n",
    "    import re\n",
    "    \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    r = urllib.request.urlopen(link).read()\n",
    "    from bs4 import BeautifulSoup \n",
    "\n",
    "    soup = BeautifulSoup(r,\"lxml\")\n",
    "    article = soup.find(\"div\",\"td-post-content\").text.strip()\n",
    "    article = article.split(\"};\\n\")\n",
    "    return article[1]\n",
    "\n",
    "contoh = 'http://www.en.netralnews.com/news/currentnews/read/19166/holding.startup.nation.summit..surabaya.set.to.become.first.creative.city.in.indonesia'\n",
    "print(netralnews_scrape(contoh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hai apa kbaar ', ' wew']\n"
     ]
    }
   ],
   "source": [
    "teks = \"Hai apa kbaar }; wew\"\n",
    "print(teks.split(\"};\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling oann.com\n",
    "def oann_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnam is making strong policy moves to bolster the development of fintech, welcoming a surge of fintech platforms and providers to service its 64 million (and growing) connected users.Adrian Chng, GoBear CEO: Fintech primed to boost financial services sector in VietnamVietnam is setting records. Notably, at the end of last year, it was named one of the world’s Top 20 internet users with 64 million connected people and in January this year, Vietnam claimed the title of the world’s fastest growing wealth hub, clocking in 210 percent wealth growth between 2007 and 2017.This is the reason that Forbes declared the country ‘every keen investor’s must-watch’ for 2018. Besides, It means that Vietnam sits at the beautiful intersection where personal finance meets inevitable innovation.In the last five years alone, Vietnam has experienced the entry of over 40 new fintech players, such as online payment platforms Mobivi and Momo, as well as the country’s first fully digital bank Timo.One of these is financial products comparison platform GoBear, one of Asia’s fastest growing startups that has recently undergone leadership renewal.Its new CEO, serial fintech entrepreneur and merchant bank Fintonia founder Adrian Chng, works closely with country director for Vietnam Bao Nguyen to increase the Vietnamese population’s direct access to financial services, with a platform that has already delivered unbiased financial comparisons to 34 million users across Asia since its launch barely three years ago.Adrian thinks it was only a matter of time before technology became primed to boost the personal financial services sector in emerging markets.The new CEO said, “It is an interesting situation because on one hand we have fast-growing and young populations with high mobile phone penetration across the region, but on the other, still relatively low penetration of traditional financial services like credit cards, loans or insurance. Because of logistical constraints, traditional distribution forms like physical bank branches cannot play in this space so it was only a matter of time before technology stepped in to improve efficiencies. When that happened, technology allowed financial institutions to access to the full spectrum of consumers.”Technology does this by making it easier for people to get connected and subsequently look for financial services online, which significantly improves customer acquisition costs. Increased connectivity and prolonged online activity also means people are now producing larger digital footprints than ever before.But how do companies even begin to harness this data, let alone make sense of it? Cue another way technology can bridge for banks and insurance companies, including on top of mining that data for precious insights, fintech is increasingly contributing to developing new models of assessment—even for the unbanked. It is the biggest opportunity yet to truly reduce the costs of acquisition.In Vietnam, GoBear has demonstrated consistent development in numerous areas throughout the past year, despite being one of the group’s newer market entries.As of today, GoBear Vietnam’s local site www.gobear.com/vn is landing over two million monthly pageviews and one million monthly users.Bao, outlining further plans for Vietnam, said, “Our key priority now is to broaden the variety of products on the GoBear Vietnam platform. We are doing this in tandem with continuously enhancing the seamless user experience, such as advanced search and filter options to help users find the best products.”He continued, “This year, GoBear Vietnam has already added home loans, home equity loans, and car loans, with life insurance and digital wallet coming soon. We are also working to integrate a personal pre-approval feature that will improve financial inclusion and financial access for our users. Our ultimate goal is to improve the standard of living for Vietnamese people who have limited access to mainstream financial services.”It is perfect timing. GoBear currently offers more than 2,000 personal finance products across its soon-to-be seven markets (it will be launching in Indonesia this coming October), including personal loans. It sees strong growth opportunities in cash-based economies, where the challenge for everyday folk is borrowing from traditional lenders.Chiming back in, Adrian shared, “Emerging economies are hotbeds for personal loans because they are teeming with businesses being formed every day. People who have started taking loans will now start looking for forms of financial protection or insurance. This is the classic curve for the life cycle of personal financial services. After that, the next level of sophistication will come from people moving up the curve and looking for wealth and asset management products, particularly in the more developed markets. We can already see a lot of fintech companies in Southeast Asian markets trying to tap into personal loans and insurance.”But the question is where does GoBear plan to fit into all of this. According to Adrian, being an internet technology innovator first and foremost means that all the data it has acquired has allowed it to deeply understand new consumer behaviours, which continue to reveal deep insights to drive product innovation, including funding new types of insurance products altogether.Adrian concluded, “Imagine a travel insurance plan that has been made just for you, using the data on your smartphone that has been harnessed from the places you have travelled, andplaces on your bucket list. Acquiring more and more data and insights every day, GoBear is perfectly placed to partner with financial institutions on their product innovations. We can help them create whole new ranges of services and products. This adds significant value to the financial services ecosystem, and it’s enabling us to expand beyond our current business model.”VIR\n"
     ]
    }
   ],
   "source": [
    "# handling vietnamnet.com\n",
    "def vietnamnet_scrape(link):\n",
    "    import ssl\n",
    "    import urllib\n",
    "    \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    r = urllib.request.urlopen(link).read()\n",
    "    from bs4 import BeautifulSoup \n",
    "\n",
    "    soup = BeautifulSoup(r,\"lxml\")\n",
    "    article = soup.find(\"div\",\"article_content\").text.strip()\n",
    "    return article\n",
    "\n",
    "contoh = 'https://english.vietnamnet.vn/fms/business/207956/fintech-primed-to-boost-financial-services-sector-in-vietnam.html'\n",
    "\n",
    "print(vietnamnet_scrape(contoh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling nytimes.com\n",
    "def nytimes_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling yale.com\n",
    "def yale_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling theedgemarkets.com\n",
    "def theedgemarkets_scrape(link):\n",
    "    article = ''\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save data\n",
    "data_startup.to_csv('data-startup-berita.csv', sep=';')\n",
    "data_incom.to_csv('data-incom.csv', sep=';')\n",
    "\n",
    "# To read data\n",
    "# data_startup = pd.read_csv('data-startup-berita.csv', sep=';')\n",
    "# data_incom = pd.read_csv('data-incom.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
